---
title: "Claude Code 서브 에이전트로 AI 기술 면접관 만들기"
description: "이직 준비가 막막했던 나는 Claude Code의 커스텀 커맨드와 3-에이전트 시스템으로 면접 준비 도구를 만들었다. 이력서 리뷰부터 3명의 AI 면접관이 독립 평가하는 시스템까지."
pubDatetime: 2026-01-24T00:00:00Z
draft: true
tags: ["ai", "claude-code", "interview", "career", "sub-agent", "automation", "custom-commands"]
---

## 목차

## 면접 준비가 막막했다

나는 프론트엔드 개발자로 경력이 쌓인 시점에 이직을 준비했다. 이력서를 쓰고, 예상 질문을 찾고, 답변을 준비하는 과정이 막막했다.

> 내 이력서가 정말 괜찮은 걸까? 면접에서 어떤 질문이 나올까? 내 답변이 충분할까?

주변 개발자들에게 물어봤지만, 각자 경험이 달라서 일관된 기준을 잡기 어려웠다. 온라인 면접 후기를 찾아보기도 했지만, 내 상황과는 맞지 않았다.

그때 나는 생각했다.

> AI한테 시키면 되지 않을까?

## Claude Code로 면접 준비 시스템을 만들기로 했다

나는 이미 Claude Code를 사용하고 있었다. 커스텀 커맨드와 서브 에이전트 기능을 써본 경험이 있었다. 그래서 이 기능들을 활용해 면접 준비 시스템을 만들기로 결심했다.

목표는 간단했다.

1. **이력서 리뷰**: 내 이력서를 객관적으로 평가받고 싶다
2. **면접 질문 생성**: 내 경력에 맞는 질문을 자동으로 만들고 싶다
3. **답변 평가**: 내 답변이 괜찮은지 다각도로 평가받고 싶다

처음에는 "프롬프트 하나면 되겠지" 생각했다. Claude Code에 "면접 질문 만들어줘"라고 했다. 결과는 기대에 못 미쳤다. 질문은 너무 일반적이었고, 평가는 단편적이었다.

> 한 번에 너무 많은 일을 시키면 안 되는구나.

나는 프로세스를 쪼개기로 했다.

## 워크플로우를 설계했다

시스템을 만들기 전에 워크플로우를 먼저 그렸다.

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│   이력서 작성    │────▶│   이력서 리뷰    │────▶│   질문지 생성    │
│  (.md/.pdf)     │     │  /review-resume │     │ /create-quest.. │
└─────────────────┘     └─────────────────┘     └─────────────────┘
                                                         │
                                                         ▼
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│   최종 보고서    │◀────│  3-에이전트 평가  │◀────│   답변 작성     │
│  _evaluation.md │     │    /evaluate    │     │  질문지에 기록   │
└─────────────────┘     └─────────────────┘     └─────────────────┘
```

각 단계를 커스텀 커맨드로 만들었다.

- `/review-resume`: 이력서를 분석하고 100점 만점으로 점수화
- `/create-questionnaire`: 이력서 기반 맞춤형 질문 생성
- `/evaluate`: 3명의 AI 면접관이 독립 평가

## 이력서 리뷰부터 시작했다

첫 번째 커맨드는 이력서 리뷰였다. 나는 `.claude/commands/review-resume.md`를 만들고 평가 기준을 정의했다.

```markdown
# 이력서 리뷰 기준

## 총 100점 배점

| 항목           | 배점 | 세부 기준                          |
| -------------- | ---- | ---------------------------------- |
| 기본 구성 요소 | 20점 | 연락처, 경력, 기술 스택 포함 여부  |
| 기술 역량 표현 | 30점 | 핵심 스택, TypeScript, 테스트 경험 |
| 성과 중심 기술 | 25점 | 정량적 성과, Before/After 표현     |
| 프로젝트 경험  | 15점 | 역할 명확화, 기술 선택 이유        |
| 가독성 및 형식 | 10점 | 길이, 형식 일관성, 오탈자          |
```

평가 기준을 `.claude/skills/resume-review.md`에 저장했다. 이렇게 하면 커맨드에서 스킬을 참조할 수 있다.

내 이력서로 테스트했다.

```bash
/review-resume resumes/caesiumy.md
```

결과는 `resumes/caesiumy_review.md`에 저장되었다.

```markdown
# 이력서 리뷰 결과

## 총점: 평균 이하

### 강점
- React/TypeScript 기반 탄탄한 기술 스택
- 프로젝트별 역할 명확히 기술

### 개선 필요 사항
- 정량적 성과 수치 부족
- 성능 최적화 경험 미기재
- Before/After 비교 부족
```

기대보다 낮았다. 하지만 피드백이 구체적이어서 무엇을 고쳐야 할지 명확했다. 나는 이력서를 수정했다.

> "React 개발" → "React 기반 대시보드 개발, 초기 로딩 시간을 크게 단축"

다시 리뷰를 받았다. 이번에는 점수가 올라갔다. 좋아졌다.

## 면접 질문지를 생성했다

두 번째 커맨드는 질문지 생성이었다. 나는 이력서를 읽고 맞춤형 질문을 만드는 로직을 작성했다.

질문 구조는 4단계로 설계했다.

```
기본 질문      →  심화 질문      →  경험 질문       →  상황 대응
"클로저란?"      "메모리 누수      "실제로 클로저     "클로저로 인한
                  가능성은?"        사용한 경험?"     버그를 만난다면?"
```

이렇게 하면 단순 암기가 아니라 깊이 있는 이해를 확인할 수 있다.

질문 데이터베이스는 `.claude/skills/interview-questionnaire.md`에 저장했다. 카테고리는 다음과 같다.

| 카테고리               | 예상 시간 | 질문 유형                                |
| ---------------------- | --------- | ---------------------------------------- |
| JavaScript/TypeScript  | 15분      | 클로저, 이벤트 루프, 비동기, 타입 시스템 |
| 프레임워크 (React/Vue) | 15분      | 상태관리, 렌더링 최적화, Hooks           |
| 웹 성능                | 10분      | Core Web Vitals, 번들 최적화             |
| 이력서 기반 질문       | 15분      | 프로젝트별 심층 질문                     |
| 협업/소프트스킬        | 10분      | 코드 리뷰, 갈등 해결, 성장               |

테스트했다.

```bash
/create-questionnaire resumes/caesiumy.md
```

결과 파일에는 여러 개의 질문이 생성되었다. 내 이력서에서 "React Query 도입"이라고 쓴 부분을 보고 이런 질문을 만들었다.

> "React Query를 도입한 이유는 무엇인가요? Redux Toolkit과 비교했을 때 어떤 장점이 있나요?"

> "서버 상태와 클라이언트 상태를 어떻게 구분하나요? 실제 프로젝트에서 어떻게 관리했나요?"

놀라웠다. 내가 생각하지 못한 질문까지 나왔다.

## 3-에이전트 시스템을 설계했다

세 번째 커맨드가 핵심이었다. 답변 평가를 어떻게 할 것인가?

나는 실제 기업 면접을 떠올렸다. 보통 2-3명의 면접관이 있다. 기술 면접관, HR, 팀 리더. 각자 다른 관점에서 평가한다.

> 한 명의 AI가 모든 걸 평가하면 편향이 생기지 않을까?

그래서 3명의 독립적인 AI 면접관을 만들기로 했다.

```
┌──────────────────────────────────────────────────────────┐
│                                                          │
│  기술 평가자           커뮤니케이션 평가자                 │
│  (엄격/비판적)         (유연/긍정적)                      │
│  [독립 서브에이전트]    [독립 서브에이전트]                │
│       │                     │                            │
│       └─────────┬───────────┘  ← 병렬 실행               │
│                 ▼                                        │
│           최종 조율자                                     │
│           (중립/균형적)                                   │
│                 │                                        │
│                 ▼                                        │
│         최종 점수 및 판정                                 │
│                                                          │
└──────────────────────────────────────────────────────────┘
```

### 기술 평가자 (Technical Evaluator)

엄격하고 비판적인 성향으로 설정했다. `.claude/agents/technical-evaluator.md`에 정의했다.

```markdown
# 기술 평가자 (Technical Evaluator)

## 성향
엄격하고 비판적, 증거 기반

## 철학
"좋은 개발자는 좋은 코드와 정확한 지식으로 증명한다"

## 평가 영역 (총 100점)
- JS/TS 역량: 25점
- 프레임워크: 25점
- 성능 최적화: 20점
- 코드 품질: 15점
- 문제 해결: 15점
```

### 커뮤니케이션 평가자 (Soft-skill Evaluator)

유연하고 긍정적인 성향으로 설정했다. `.claude/agents/communication-evaluator.md`에 정의했다.

```markdown
# 커뮤니케이션 평가자

## 성향
유연하고 긍정적, 잠재력 중심

## 철학
"완벽한 지원자는 없다. 함께 성장할 동료를 찾는다"

## 평가 영역 (총 100점)
- 커뮤니케이션: 25점
- 협업 역량: 25점
- 성장 의지: 20점
- 문화 적합성: 15점
- 주도성: 15점
```

### 최종 조율자 (Final Arbiter)

두 평가를 종합하는 역할이다. `.claude/agents/final-arbiter.md`에 정의했다.

```markdown
# 최종 조율자

## 역할
- 두 평가자의 의견 종합
- 불일치 조율
- 가중치 적용 (기술 45%, 소프트스킬 30%, 성장성 25%)
- 최종 판정
```

## Anchoring Bias를 방지했다

중요한 설계 결정이 있었다. **기술 평가자와 커뮤니케이션 평가자를 서브 에이전트로 분리했다.**

왜 분리했을까? Anchoring Bias 때문이다. 한 명이 먼저 평가하면, 다음 평가자가 그 평가에 영향을 받는다. "기술 점수가 높네? 그럼 나도 비슷하게 줘야지" 같은 편향이 생긴다.

서브 에이전트로 분리하면 각자 독립적으로 실행된다. 서로의 평가를 보지 않는다. 그래서 순수하게 자기 관점에서만 평가한다.

Claude Code의 서브 에이전트는 병렬로 실행된다. 두 평가자가 동시에 작업하고, 최종 조율자가 결과를 받아서 종합한다.

> 이렇게 하면 더 공정하지 않을까?

## 실제로 사용해봤다

질문지에 답변을 작성했다. STAR 기법을 활용했다.

```
Situation (상황): 어떤 상황이었나요?
Task (과제): 어떤 문제를 해결해야 했나요?
Action (행동): 어떤 조치를 취했나요?
Result (결과): 어떤 결과를 얻었나요?
```

예를 들어, "성능 최적화 경험이 있나요?" 질문에 이렇게 답했다.

> "이전 프로젝트에서(S) 대시보드 페이지 로딩이 오래 걸리는 문제가 있었습니다(T). 코드 스플리팅과 이미지 lazy loading을 적용했고(A), 로딩 시간을 크게 단축했습니다(R)."

답변을 다 쓰고 평가를 실행했다.

```bash
/evaluate resumes/caesiumy_questionnaire.md
```

기다리는 동안 서브 에이전트가 병렬로 실행되는 것을 볼 수 있었다. 기술 평가자와 커뮤니케이션 평가자가 동시에 작업했다.

## 첫 평가 결과는 합격 판정이었다

최종 보고서가 생성되었다.

```markdown
# 최종 평가 보고서

## 종합 점수: 합격권

### 기술 평가자 점수
- JS/TS 역량: 클로저 개념은 정확하나 실무 활용 사례 부족
- 프레임워크: React 이해도 양호, 상태관리 심화 부족
- 성능 최적화: 구체적 경험 있음, 측정 방법 설명 부족

### 커뮤니케이션 평가자 점수
- 커뮤니케이션: STAR 기법 잘 활용, 더 구체적일 필요
- 협업 역량: 코드 리뷰 경험 좋음
- 성장 의지: 학습 의지 보임

### 최종 판정: 합격
채용 권고. 즉시 업무 투입 가능.
```

합격 판정이었다. 하지만 나는 더 잘할 수 있다고 생각했다.

피드백을 읽었다. "성능 최적화 측정 방법 설명 부족"이라는 지적이 있었다. 나는 답변을 수정했다.

> "Lighthouse와 Chrome DevTools Performance 탭으로 측정했고, FCP(First Contentful Paint)를 크게 개선했습니다."

다시 평가를 받았다. 이번에는 점수가 더 올라갔다.

## 실제 면접에서 통과했다

나는 이 시스템으로 한동안 연습했다. 질문지를 계속 생성하고, 답변을 쓰고, 평가받고, 고치고. 점수는 점진적으로 향상되었다.

그리고 실제 면접을 봤다. 놀랍게도 AI가 만든 질문 중 상당수가 실제로 나왔다.

> "React의 렌더링 최적화 경험이 있나요?"

> "TypeScript의 제네릭을 실무에서 어떻게 활용했나요?"

> "팀원과 의견 충돌이 있었던 경험을 말씀해주세요."

나는 이미 연습한 답변을 자신 있게 말했다. 면접관들이 고개를 끄덕였다.

**기술 면접을 통과했다.**

## 시스템을 공개했다

나는 이 시스템을 GitHub에 공개했다. 다른 개발자들도 이직 준비에 도움받길 바랐다.

[https://github.com/CaesiumY/claude-interview-agents](https://github.com/CaesiumY/claude-interview-agents)

프로젝트 구조는 다음과 같다.

```
interview-agents/
├── .claude/
│   ├── commands/          # 3개 커스텀 커맨드
│   │   ├── review-resume.md
│   │   ├── create-questionnaire.md
│   │   └── evaluate.md
│   ├── agents/           # 3명의 평가 에이전트
│   │   ├── technical-evaluator.md
│   │   ├── communication-evaluator.md
│   │   └── final-arbiter.md
│   └── skills/           # 평가 기준 및 질문 DB
│       ├── resume-review.md
│       └── interview-questionnaire.md
├── resumes/              # 이력서 및 결과물
│   └── sample_resume.md
└── README.md
```

사용 방법은 간단하다.

```bash
# 1. 이력서 리뷰
/review-resume resumes/your-name.md

# 2. 질문지 생성
/create-questionnaire resumes/your-name.md

# 3. 답변 작성 후 평가
/evaluate resumes/your-name_questionnaire.md
```

## 다른 직군에도 적용할 수 있다

현재는 프론트엔드에 특화되어 있지만, 스킬 파일만 수정하면 다른 직군에도 사용할 수 있다.

백엔드 개발자라면 `.claude/skills/interview-questionnaire.md`에 데이터베이스, API 설계, 동시성 제어 관련 질문을 추가하면 된다.

디자이너라면 포트폴리오 리뷰, 디자인 시스템, UX 리서치 질문으로 바꿀 수 있다.

핵심은 **워크플로우 설계와 3-에이전트 시스템**이다. 이 두 가지만 이해하면 어떤 분야든 응용할 수 있다.

## 배운 점들

이 프로젝트를 진행하면서 몇 가지를 배웠다.

### 1. AI에게 한 번에 너무 많은 일을 시키지 마라

처음에는 "면접 준비 해줘"라고 했다. 결과는 좋지 않았다. 작업을 쪼개니 훨씬 좋아졌다.

### 2. 서브 에이전트로 편향을 줄일 수 있다

한 명의 AI가 모든 걸 평가하면 편향이 생긴다. 독립적인 서브 에이전트로 분리하면 더 공정하다.

### 3. 피드백 루프가 중요하다

평가 → 수정 → 재평가 사이클을 반복하면 점수가 올라간다. 나는 점진적으로 점수를 높였다.

### 4. 구조화된 답변이 강력하다

STAR 기법처럼 구조화된 답변 방식을 사용하면 평가받기 쉽다. AI도, 실제 면접관도 선호한다.

### 5. Claude Code의 커스텀 커맨드는 강력하다

커맨드 하나로 복잡한 워크플로우를 실행할 수 있다. 스킬 파일과 서브 에이전트를 조합하면 무궁무진하다.

## 마무리

나는 막막했던 면접 준비를 AI로 체계화했다. 이력서 리뷰, 맞춤형 질문 생성, 3-에이전트 평가 시스템을 만들었다.

이 시스템 덕분에 기술 면접을 통과했다. AI가 만든 질문 중 상당수가 실제로 나왔고, 연습한 답변을 자신 있게 말할 수 있었다.

레포지토리를 공개했으니 누구나 사용할 수 있다. 이직을 준비하는 개발자들에게 도움이 되길 바란다.

다음에는 이 시스템을 더 발전시켜서 모의 면접 대화 기능도 추가해볼 예정이다. 궁금한 점은 댓글로 남겨주세요!

## 참고 자료

- [claude-interview-agents GitHub 레포지토리](https://github.com/CaesiumY/claude-interview-agents)
- [Claude Code 공식 문서](https://docs.anthropic.com/en/docs/overview)
- [STAR 면접 기법 가이드](https://www.indeed.com/career-advice/interviewing/how-to-use-the-star-interview-response-technique)
