---
title: "Claude Code 서브 에이전트로 AI 기술 면접관 만들기"
description: "이직 준비가 막막했던 나는 Claude Code의 커스텀 커맨드와 3-에이전트 시스템으로 면접 준비 도구를 만들었다. 이력서 리뷰부터 3명의 AI 면접관이 독립 평가하는 시스템까지."
pubDatetime: 2026-01-24T00:00:00Z
draft: false
tags: ["ai", "claude-code", "interview", "career", "sub-agent", "automation", "custom-commands"]
---

## 목차

## 면접 준비가 막막했다

나는 프론트엔드 개발자로 경력이 쌓인 시점에 이직을 준비했다. 이력서를 쓰고, 예상 질문을 찾고, 답변을 준비하는 과정이 막막했다.

> 내 이력서가 정말 괜찮은 걸까? 면접에서 어떤 질문이 나올까? 내 답변이 충분할까?

주변 개발자들에게 물어봤지만, 각자 경험이 달라서 일관된 기준을 잡기 어려웠다. 온라인 면접 후기를 찾아보기도 했지만, 내 상황과는 맞지 않았다.

그때 나는 생각했다.

> AI한테 시키면 되지 않을까?

## Claude Code로 면접 준비 시스템을 만들기로 했다

나는 이미 Claude Code를 사용하고 있었다. 커스텀 커맨드와 서브 에이전트 기능을 써본 경험이 있었다. 그래서 이 기능들을 활용해 면접 준비 시스템을 만들기로 결심했다.

목표는 간단했다.

1. **이력서 리뷰**: 내 이력서를 객관적으로 평가받고 싶다
2. **면접 질문 생성**: 내 경력에 맞는 질문을 자동으로 만들고 싶다
3. **답변 평가**: 내 답변이 괜찮은지 다각도로 평가받고 싶다

처음에는 "프롬프트 하나면 되겠지" 생각했다. Claude Code에 "면접 질문 만들어줘"라고 했다. 결과는 기대에 못 미쳤다. 질문은 너무 일반적이었고, 평가는 단편적이었다.

> 한 번에 너무 많은 일을 시키면 안 되는구나.

나는 프로세스를 쪼개기로 했다.

## 워크플로우를 설계했다

시스템을 만들기 전에 워크플로우를 먼저 그렸다.

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│   이력서 작성    │────▶│   이력서 리뷰    │────▶│   질문지 생성    │
│  (.md/.pdf)     │     │  /review-resume │     │ /create-quest.. │
└─────────────────┘     └─────────────────┘     └─────────────────┘
                                                         │
                                                         ▼
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│   최종 보고서    │◀────│  3-에이전트 평가  │◀────│   답변 작성     │
│  _evaluation.md │     │    /evaluate    │     │  질문지에 기록   │
└─────────────────┘     └─────────────────┘     └─────────────────┘
```

각 단계를 커스텀 커맨드로 만들었다.

- `/review-resume`: 이력서를 분석하고 100점 만점으로 점수화
- `/create-questionnaire`: 이력서 기반 맞춤형 질문 생성
- `/evaluate`: 3명의 AI 면접관이 독립 평가

## 이력서 리뷰부터 시작했다

첫 번째 커맨드는 이력서 리뷰였다. 나는 `.claude/commands/review-resume.md`를 만들고 평가 기준을 정의했다.

```markdown
# 이력서 리뷰 기준

## 총 100점 배점

| 항목           | 배점 | 세부 기준                          |
| -------------- | ---- | ---------------------------------- |
| 기본 구성 요소 | 20점 | 연락처, 경력, 기술 스택 포함 여부  |
| 기술 역량 표현 | 30점 | 핵심 스택, TypeScript, 테스트 경험 |
| 성과 중심 기술 | 25점 | 정량적 성과, Before/After 표현     |
| 프로젝트 경험  | 15점 | 역할 명확화, 기술 선택 이유        |
| 가독성 및 형식 | 10점 | 길이, 형식 일관성, 오탈자          |
```

평가 기준을 `.claude/skills/resume-review.md`에 저장했다. 이렇게 하면 커맨드에서 스킬을 참조할 수 있다.

## 면접 질문지를 생성했다

두 번째 커맨드는 질문지 생성이었다. 나는 이력서를 읽고 맞춤형 질문을 만드는 로직을 작성했다.

질문 구조는 4단계로 설계했다.

```
기본 질문      →  심화 질문      →  경험 질문       →  상황 대응
"클로저란?"      "메모리 누수      "실제로 클로저     "클로저로 인한
                  가능성은?"        사용한 경험?"     버그를 만난다면?"
```

이렇게 하면 단순 암기가 아니라 깊이 있는 이해를 확인할 수 있다.

질문 데이터베이스는 `.claude/skills/interview-questionnaire.md`에 저장했다. 카테고리는 다음과 같다.

| 카테고리               | 예상 시간 | 질문 유형                                |
| ---------------------- | --------- | ---------------------------------------- |
| JavaScript/TypeScript  | 15분      | 클로저, 이벤트 루프, 비동기, 타입 시스템 |
| 프레임워크 (React/Vue) | 15분      | 상태관리, 렌더링 최적화, Hooks           |
| 웹 성능                | 10분      | Core Web Vitals, 번들 최적화             |
| 이력서 기반 질문       | 15분      | 프로젝트별 심층 질문                     |
| 협업/소프트스킬        | 10분      | 코드 리뷰, 갈등 해결, 성장               |

이력서에 적힌 기술 스택과 프로젝트 경험을 분석해서, 그에 맞는 심층 질문을 자동으로 생성한다.

## 3-에이전트 시스템을 설계했다

세 번째 커맨드가 핵심이었다. 답변 평가를 어떻게 할 것인가?

나는 실제 기업 면접을 떠올렸다. 보통 2-3명의 면접관이 있다. 기술 면접관, HR, 팀 리더. 각자 다른 관점에서 평가한다.

> 한 명의 AI가 모든 걸 평가하면 편향이 생기지 않을까?

그래서 3명의 독립적인 AI 면접관을 만들기로 했다.

```
┌──────────────────────────────────────────────────────────┐
│                                                          │
│  기술 평가자           커뮤니케이션 평가자                 │
│  (엄격/비판적)         (유연/긍정적)                      │
│  [독립 서브에이전트]    [독립 서브에이전트]                │
│       │                     │                            │
│       └─────────┬───────────┘  ← 병렬 실행               │
│                 ▼                                        │
│           최종 조율자                                     │
│           (중립/균형적)                                   │
│                 │                                        │
│                 ▼                                        │
│         최종 점수 및 판정                                 │
│                                                          │
└──────────────────────────────────────────────────────────┘
```

### 기술 평가자 (Technical Evaluator)

엄격하고 비판적인 성향으로 설정했다. `.claude/agents/technical-evaluator.md`에 정의했다.

```markdown
# 기술 평가자 (Technical Evaluator)

## 성향
엄격하고 비판적, 증거 기반

## 철학
"좋은 개발자는 좋은 코드와 정확한 지식으로 증명한다"

## 평가 영역 (총 100점)
- JS/TS 역량: 25점
- 프레임워크: 25점
- 성능 최적화: 20점
- 코드 품질: 15점
- 문제 해결: 15점
```

### 커뮤니케이션 평가자 (Soft-skill Evaluator)

유연하고 긍정적인 성향으로 설정했다. `.claude/agents/communication-evaluator.md`에 정의했다.

```markdown
# 커뮤니케이션 평가자

## 성향
유연하고 긍정적, 잠재력 중심

## 철학
"완벽한 지원자는 없다. 함께 성장할 동료를 찾는다"

## 평가 영역 (총 100점)
- 커뮤니케이션: 25점
- 협업 역량: 25점
- 성장 의지: 20점
- 문화 적합성: 15점
- 주도성: 15점
```

### 최종 조율자 (Final Arbiter)

두 평가를 종합하는 역할이다. `.claude/agents/final-arbiter.md`에 정의했다.

```markdown
# 최종 조율자

## 역할
- 두 평가자의 의견 종합
- 불일치 조율
- 가중치 적용 (기술 45%, 소프트스킬 30%, 성장성 25%)
- 최종 판정
```

## Anchoring Bias를 방지했다

중요한 설계 결정이 있었다. **기술 평가자와 커뮤니케이션 평가자를 서브 에이전트로 분리했다.**

왜 분리했을까? Anchoring Bias 때문이다. 한 명이 먼저 평가하면, 다음 평가자가 그 평가에 영향을 받는다. "기술 점수가 높네? 그럼 나도 비슷하게 줘야지" 같은 편향이 생긴다.

서브 에이전트로 분리하면 각자 독립적으로 실행된다. 서로의 평가를 보지 않는다. 그래서 순수하게 자기 관점에서만 평가한다.

Claude Code의 서브 에이전트는 병렬로 실행된다. 두 평가자가 동시에 작업하고, 최종 조율자가 결과를 받아서 종합한다.

> 이렇게 하면 더 공정하지 않을까?

## 실제 기술 면접에서 통과했다

나는 이 시스템을 사용해 맞닥뜨린 기술 면접을 준비했다. 그리고 놀랍게도 AI가 만든 질문 중 상당수가 실제로 나왔다. 그 덕분에 나는 올바른 답변을 할 수 있었고, 기술 면접에 통과했다.

## 시스템을 공개했다

나는 이 시스템을 GitHub에 공개했다. 다른 개발자들도 이직 준비에 도움받길 바랐다.

[https://github.com/CaesiumY/claude-interview-agents](https://github.com/CaesiumY/claude-interview-agents)

프로젝트 구조는 다음과 같다.

```
interview-agents/
├── .claude/
│   ├── commands/          # 3개 커스텀 커맨드
│   │   ├── review-resume.md
│   │   ├── create-questionnaire.md
│   │   └── evaluate.md
│   ├── agents/           # 3명의 평가 에이전트
│   │   ├── technical-evaluator.md
│   │   ├── communication-evaluator.md
│   │   └── final-arbiter.md
│   └── skills/           # 평가 기준 및 질문 DB
│       ├── resume-review.md
│       └── interview-questionnaire.md
├── resumes/              # 이력서 및 결과물
│   └── sample_resume.md
└── README.md
```

사용 방법은 간단하다. 굳이 마크다운 파일이 아니어도 된다. PDF도 가능!

```bash
# 1. 이력서 리뷰
/review-resume resumes/your-name.md

# 2. 질문지 생성
/create-questionnaire resumes/your-name.md

# 3. 답변 작성 후 평가
/evaluate resumes/your-name_questionnaire.md
```

## 다른 직군에도 적용할 수 있다

현재는 프론트엔드에 특화되어 있지만, 스킬 파일만 수정하면 다른 직군에도 사용할 수 있다.

백엔드 개발자라면 `.claude/skills/interview-questionnaire.md`에 데이터베이스, API 설계, 동시성 제어 관련 질문을 추가하면 된다.

디자이너라면 포트폴리오 리뷰, 디자인 시스템, UX 리서치 질문으로 바꿀 수 있다.

핵심은 **워크플로우 설계와 3-에이전트 시스템**이다. 이 두 가지만 이해하면 어떤 분야든 응용할 수 있다.

## 마무리

나는 기술 면접 준비를 AI로 체계화했다. 이력서 리뷰, 맞춤형 질문 생성, 3-에이전트 평가 시스템을 만들었다.

이 시스템 덕분에 기술 면접을 통과했다. AI가 만든 질문이 실제로 나왔고, 연습한 답변을 자신 있게 말할 수 있었다.

레포지토리를 공개했으니 누구나 사용할 수 있다. 이직을 준비하는 개발자들에게 도움이 되길 바란다.

다음에는 이 시스템을 발전 시켜서 특정 회사의 자료 조사 후에 컬쳐핏 면접 준비도 도와주는 시스템을 만들까 생각 중이다.

## 참고 자료

- [claude-interview-agents GitHub 레포지토리](https://github.com/CaesiumY/claude-interview-agents)
- [Claude Code 공식 문서](https://docs.anthropic.com/en/docs/overview)